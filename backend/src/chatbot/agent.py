"""
LangGraph Agent for Storm Tracker Chatbot
Uses Gemini as LLM and integrates RAG + Rescue Request tools
"""
from typing import TypedDict, Annotated, Sequence
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages
from src.config import config
from src.chatbot.tools import CHATBOT_TOOLS
from src.logger import logger
import asyncio


# Define the state for our agent
class AgentState(TypedDict):
    """State of the chatbot agent"""
    messages: Annotated[Sequence[BaseMessage], add_messages]


class StormChatbotAgent:
    """
    Chatbot Agent for Storm Tracker
    - Answers questions about storms, preparation, first aid, rescue
    - Can create rescue requests when needed
    """
    
    def __init__(self):
        """Initialize the chatbot agent with LangGraph"""
        # Initialize Gemini LLM with tools
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=config.GOOGLE_API_KEY,
            temperature=0.7,
            max_tokens=2048,
        )
        
        # Bind tools to LLM
        self.llm_with_tools = self.llm.bind_tools(CHATBOT_TOOLS)
        
        # Create the graph
        self.graph = self._create_graph()
    
    def _create_graph(self) -> StateGraph:
        """Create the LangGraph workflow"""
        # Define workflow
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("agent", self._call_model)
        workflow.add_node("tools", ToolNode(CHATBOT_TOOLS))
        
        # Set entry point
        workflow.set_entry_point("agent")
        
        # Add conditional edges
        workflow.add_conditional_edges(
            "agent",
            self._should_continue,
            {
                "continue": "tools",
                "end": END
            }
        )
        
        # Add edge from tools back to agent
        workflow.add_edge("tools", "agent")
        
        # Compile the graph
        return workflow.compile()
    
    async def _call_model(self, state: AgentState) -> AgentState:
        """Call the LLM with current state"""
        messages = state["messages"]
        
        # Add system message for context
        system_message = """B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh c·ªßa h·ªá th·ªëng Storm Tracker, chuy√™n h·ªó tr·ª£ ng∆∞·ªùi d√¢n v·ªÅ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn b√£o.

NHI·ªÜM V·ª§ C·ª¶A B·∫†N:
1. Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ b√£o, c√°ch ph√≤ng tr√°nh, chu·∫©n b·ªã ƒë√≥n b√£o
2. Cung c·∫•p ki·∫øn th·ª©c s∆° c·ª©u v√† c·ª©u h·ªô
3. Cung c·∫•p th√¥ng tin theo d√µi b√£o, thi·ªát h·∫°i v√† t√¨nh h√¨nh c·ª©u h·ªô
4. T·∫°o y√™u c·∫ßu c·ª©u h·ªô kh·∫©n c·∫•p khi ng∆∞·ªùi d√πng c·∫ßn gi√∫p ƒë·ª°

C√îNG C·ª§ B·∫†N C√ì:
- search_storm_knowledge: T√¨m ki·∫øm ki·∫øn th·ª©c trong c∆° s·ªü d·ªØ li·ªáu v·ªÅ b√£o, ph√≤ng tr√°nh, s∆° c·ª©u
- create_rescue_request: T·∫°o y√™u c·∫ßu c·ª©u h·ªô kh·∫©n c·∫•p m·ªõi
- get_storm_info: L·∫•y th√¥ng tin chi ti·∫øt v·ªÅ c∆°n b√£o (t√™n, th·ªùi gian, m√¥ t·∫£)
- get_storm_tracking: L·∫•y d·ªØ li·ªáu theo d√µi v·ªã tr√≠ v√† c∆∞·ªùng ƒë·ªô b√£o theo th·ªùi gian
- get_damage_info: L·∫•y th√¥ng tin thi·ªát h·∫°i chi ti·∫øt theo t·ª´ng ƒë·ªãa ƒëi·ªÉm
- get_rescue_requests: Xem danh s√°ch c√°c y√™u c·∫ßu c·ª©u h·ªô (c√≥ th·ªÉ l·ªçc theo b√£o, tr·∫°ng th√°i, m·ª©c ∆∞u ti√™n)

H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG TOOLS:
- Khi h·ªèi v·ªÅ ki·∫øn th·ª©c (c√°ch chu·∫©n b·ªã, s∆° c·ª©u): d√πng search_storm_knowledge
- Khi h·ªèi v·ªÅ th√¥ng tin b√£o (t√™n, th·ªùi gian): d√πng get_storm_info
- Khi h·ªèi v·ªÅ v·ªã tr√≠, ƒë∆∞·ªùng ƒëi, c∆∞·ªùng ƒë·ªô b√£o: d√πng get_storm_tracking
- Khi h·ªèi v·ªÅ thi·ªát h·∫°i, m·ª©c ƒë·ªô thi·ªát h·∫°i: d√πng get_damage_info
- Khi h·ªèi v·ªÅ t√¨nh h√¨nh c·ª©u h·ªô, danh s√°ch c·∫ßn c·ª©u: d√πng get_rescue_requests
- Khi ng∆∞·ªùi d√πng c·∫ßn c·ª©u h·ªô kh·∫©n c·∫•p: thu th·∫≠p th√¥ng tin ƒë·∫ßy ƒë·ªß r·ªìi d√πng create_rescue_request

QUY T·∫ÆC:
- Lu√¥n th√¢n thi·ªán, l·ªãch s·ª± v√† ƒë·ªìng c·∫£m
- ∆Øu ti√™n an to√†n c·ªßa ng∆∞·ªùi d√¢n l√™n h√†ng ƒë·∫ßu
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, r√µ r√†ng
- N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y th·ª´a nh·∫≠n v√† ƒë·ªÅ ngh·ªã ng∆∞·ªùi d√πng li√™n h·ªá ƒë∆∞·ªùng d√¢y n√≥ng kh·∫©n c·∫•p
- Khi c√≥ nhi·ªÅu tool c√≥ th·ªÉ d√πng, h√£y ch·ªçn tool ph√π h·ª£p nh·∫•t v·ªõi c√¢u h·ªèi

FORMAT MARKDOWN - QUAN TR·ªåNG:
B·∫†N PH·∫¢I TR·∫¢ L·ªúI B·∫∞NG MARKDOWN FORMAT ƒê·ªÇ FRONTEND D·ªÑ HI·ªÇN TH·ªä:

1. **Ti√™u ƒë·ªÅ**: D√πng # ## ### cho c√°c c·∫•p ti√™u ƒë·ªÅ
   V√≠ d·ª•: ## C√°ch chu·∫©n b·ªã ƒë√≥n b√£o

2. **Danh s√°ch**: D√πng - ho·∫∑c s·ªë th·ª© t·ª± 1. 2. 3.
   V√≠ d·ª•:
   - M·ª•c 1
   - M·ª•c 2
   
   Ho·∫∑c:
   1. B∆∞·ªõc ƒë·∫ßu ti√™n
   2. B∆∞·ªõc th·ª© hai

3. **In ƒë·∫≠m**: D√πng **text** ho·∫∑c __text__
   V√≠ d·ª•: **Quan tr·ªçng**: Ph·∫£i s∆° t√°n ngay

4. **In nghi√™ng**: D√πng *text* ho·∫∑c _text_
   V√≠ d·ª•: *L∆∞u √Ω*: C·∫ßn theo d√µi th∆∞·ªùng xuy√™n

5. **Code ho·∫∑c highlight**: D√πng `text`
   V√≠ d·ª•: G·ªçi s·ªë `115` ƒë·ªÉ c·ª©u h·ªô

6. **ƒê∆∞·ªùng k·∫ª ngang**: D√πng --- ho·∫∑c ___

7. **Link**: D√πng [text](url)

8. **Nh·∫•n m·∫°nh kh·∫©n c·∫•p**: D√πng > cho blockquote
   V√≠ d·ª•:
   > ‚ö†Ô∏è **C·∫¢NH B√ÅO KH·∫®N C·∫§P**: C·∫ßn s∆° t√°n ngay l·∫≠p t·ª©c!

9. **B·∫£ng** (n·∫øu c·∫ßn):
   | C·ªôt 1 | C·ªôt 2 |
   |-------|-------|
   | D·ªØ li·ªáu 1 | D·ªØ li·ªáu 2 |

10. **Xu·ªëng d√≤ng**: D√πng 2 spaces ho·∫∑c <br> ·ªü cu·ªëi d√≤ng, ho·∫∑c ƒë·ªÉ 1 d√≤ng tr·ªëng

V√ç D·ª§ TR·∫¢ L·ªúI ƒê√öNG FORMAT:

## C√°ch chu·∫©n b·ªã ƒë√≥n b√£o üå™Ô∏è

### 1. Tr∆∞·ªõc khi b√£o ƒë·ªï b·ªô

**C·∫ßn l√†m ngay:**
- Theo d√µi tin t·ª©c v·ªÅ b√£o th∆∞·ªùng xuy√™n
- Chu·∫©n b·ªã ƒë·ªì d√πng thi·∫øt y·∫øu:
  - N∆∞·ªõc u·ªëng (ƒë·ªß 3-5 ng√†y)
  - Th·ª±c ph·∫©m kh√¥, ƒë·ªì h·ªôp
  - Thu·ªëc men, v·∫≠t d·ª•ng y t·∫ø
  - ƒê√®n pin, pin d·ª± ph√≤ng
  - Radio ƒë·ªÉ nghe tin

### 2. Gia c·ªë nh√† c·ª≠a

> ‚ö†Ô∏è **L∆∞u √Ω**: Ph·∫£i ho√†n th√†nh vi·ªác gia c·ªë tr∆∞·ªõc 24 gi·ªù khi b√£o ƒë·ªï b·ªô!

1. Ki·ªÉm tra m√°i nh√†, c·ª≠a s·ªï
2. D√πng v√°n g·ªó che c·ª≠a s·ªï
3. Thu gom ƒë·ªì ƒë·∫°c ngo√†i s√¢n v√†o trong

---

**ƒê∆∞·ªùng d√¢y n√≥ng kh·∫©n c·∫•p**: `115` (C·ª©u h·ªô c·ª©u n·∫°n)

H√ÉY LU√îN FORMAT TR·∫¢ L·ªúI C·ª¶A B·∫†N THEO CHU·∫®N MARKDOWN NH∆Ø V√ç D·ª§ TR√äN!"""
        
        # Prepare messages with system context
        all_messages = [HumanMessage(content=system_message)] + list(messages)
        
        # Call LLM asynchronously
        response = await asyncio.to_thread(self.llm_with_tools.invoke, all_messages)
        
        return {"messages": [response]}
    
    def _should_continue(self, state: AgentState) -> str:
        """Determine if we should continue to tools or end"""
        messages = state["messages"]
        last_message = messages[-1]
        logger.debug(f"Last message type: {type(last_message)}")
        # If there are tool calls, continue to tools node
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            logger.debug("Continuing to tools node")
            logger.debug(f"Tool calls: {last_message.tool_calls}")
            return "continue"
        
        # Otherwise, end
        return "end"
    
    async def chat(self, message: str, conversation_history: list = None) -> dict:
        """
        Process a chat message and return response
        
        Args:
            message: User's message
            conversation_history: Previous conversation messages
            
        Returns:
            Dictionary with response and updated history
        """
        # Prepare initial state
        messages = conversation_history or []
        messages.append(HumanMessage(content=message))
        
        initial_state = {"messages": messages}
        
        # Run the graph
        result = await self.graph.ainvoke(initial_state)
        
        # Get the final response
        final_messages = result["messages"]
        last_message = final_messages[-1]
        
        # Extract response content
        if isinstance(last_message, AIMessage):
            response_text = last_message.content
        else:
            response_text = str(last_message)
        
        return {
            "response": response_text,
            "conversation_history": final_messages
        }


# Create singleton instance
chatbot_agent = StormChatbotAgent()
